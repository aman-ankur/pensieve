# Pensieve Configuration File
# AI-powered meeting transcript summarization tool

# Monitoring Settings
monitoring:
  zoom_folder: "~/Documents/Zoom"
  file_extensions: [".txt"]
  ignored_folders: ["archive", "temp", ".git"]
  poll_interval: 2.0  # seconds
  watch_patterns: 
    - "*/meeting_saved_closed_caption.txt"
  min_file_size: 1024  # Skip files smaller than 1KB
  check_interval: 2    # Seconds between checks
  file_stable_time: 3  # Wait time to ensure file is complete

# AI Processing Settings
processing:
  ollama_url: "http://localhost:11434"
  model_name: "llama3.1:8b"       # Fallback/default model
  max_retries: 3
  retry_delay: 5       # Seconds between retries
  request_timeout: 120  # seconds
  chunk_size: 4000
  max_chunk_overlap: 200
  chunk_model: "llama3.1:8b"      # Use better model for chunk processing
  synthesis_model: "llama3.1:8b"  # Better model for final synthesis
  
  # Chunking settings for large transcripts
  chunking:
    enabled: true
    max_chunk_size: 2000  # Reduced from 3000 for faster processing
    overlap_size: 100     # Small overlap between chunks

# Output Settings
output:
  summaries_folder: "./summaries"
  template_file: "config/prompts/summary_template.txt"
  date_format: "%Y-%m-%d"
  time_format: "%H-%M"
  filename_format: "{date}_{time}_{title}_summary.md"
  include_metadata: true
  create_backups: false
  organize_by_date: true
  create_monthly_folders: true
  
# Logging Settings  
logging:
  level: "INFO"               # DEBUG, INFO, WARNING, ERROR
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  file_rotation: true
  log_file: "./logs/pensieve.log"
  max_file_size: "10MB"
  backup_count: 5
  console_output: true

# Feature Flags
features:
  multilingual: true           # Handle Hindi/English mixed content
  action_extraction: true      # Extract action items
  priority_detection: true     # Detect priority levels
  calendar_integration: false  # Future feature
  slack_notifications: false  # Future feature
  cloud_fallback: true         # Allow fallback to cloud when local fails
  quality_assessment: true     # Assess summary quality and suggest improvements
  adaptive_chunking: true      # Smart chunking based on content structure
  
# Performance Settings
performance:
  max_concurrent_files: 3     # Process multiple files simultaneously
  memory_limit: "4GB"         # Maximum memory usage
  cpu_limit: 80              # Maximum CPU usage percentage
  cache_summaries: true      # Cache processed summaries
  claude_rate_limit: 50       # Requests per minute for Claude
  ollama_concurrent_limit: 2  # Max concurrent Ollama requests

# Meeting Classification
meeting_types:
  one_on_one:
    keywords: ["1:1", "one-on-one", "weekly sync", "catch up", "check in"]
    priority: "high"
    preferred_provider: "claude"
  team_meeting:
    keywords: ["team", "standup", "scrum", "daily", "retrospective"]
    priority: "medium"
    preferred_provider: "auto"
  alignment:
    keywords: ["alignment", "sync", "review", "planning", "strategy"]
    priority: "high"
    preferred_provider: "claude"  # Strategic meetings get best AI
  interview:
    keywords: ["interview", "fit", "coding", "system design"]
    priority: "low"
    preferred_provider: "ollama"  # Local processing for candidate privacy
  all_hands:
    keywords: ["all hands", "company", "quarterly", "town hall"]
    priority: "low"
    preferred_provider: "ollama"

# AI Providers Configuration (for hybrid processor)
ai_providers:
  claude:
    enabled: true   # API key loaded from environment variable
    model_name: "claude-3-haiku-20240307"  # 12x cheaper than Sonnet
    api_key: null  # Set your Anthropic API key here or use environment variable ANTHROPIC_API_KEY
    max_tokens: 2000  # Increased for enhanced template with more detail
    temperature: 0.1
    timeout: 120
    priority: 1  # Highest priority (lowest number)
    privacy_mode: false  # If true, will not send data to cloud unless explicitly enabled per meeting
    
  ollama:
    enabled: true
    model_name: "llama3.1:8b"
    api_url: "http://localhost:11434"
    temperature: 0.1
    timeout: 120
    priority: 2  # Fallback priority
    two_pass_enabled: true    # Enable two-pass processing for better quality
    persona_prompting: true   # Use persona-based prompts
    entity_extraction: true   # Extract entities first, then summarize
    options:
      top_k: 40
      top_p: 0.9
      repeat_penalty: 1.1
      num_ctx: 8192  # Context window size

# Quality Thresholds (for hybrid processor)  
quality:
  min_action_items: 0      # Minimum number of action items expected
  min_technical_terms: 3   # Minimum technical terms for engineering meetings
  min_summary_length: 200  # Minimum characters in summary
  scoring:
    technical_content: 0.4    # Weight for technical content detection
    action_items: 0.3         # Weight for action item extraction
    business_context: 0.2     # Weight for business context capture
    clarity: 0.1             # Weight for summary clarity 